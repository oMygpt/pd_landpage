主题：Paper Daily 2025/11/04

日期：2025年11月4日星期二中国标准时间上午1:02:42

从： PaperDaily

至：alone

# 今日概览

- 当前论文集中探讨了智能体（Agent）与模型记忆的多个核心维度：从记忆的几何表征、低秩结构、动态几何建模到可解释的遗忘机制。多数研究聚焦于大语言模型（LLM）与序列模型中的记忆本质，提出“几何记忆”作为对传统关联记忆的范式升级。  
- 记忆的可塑性与可控性成为关键议题，包括通过FIM实现高效并行遗忘、利用动作驱动过程建模连续时间记忆，以及通过低秩结构实现高效知识重用。  
- 代表性主题包括：几何记忆（Geometric Memory）、低秩结构（Low-Rank Structure）、可解释遗忘（Explainable Unlearning）、动态几何建模（Adaptive Geometry）、动作驱动记忆（Action-Driven Memory）、记忆与控制的统一（Memory-Control Integration）。

# 优先阅读推荐

1. Deep sequence models tend to memorize geometrically; it is unclear why

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/b873f2be2284b50a18a58f6c3cac544650ef9c1484b0a566454d6f66cf7081d6.jpg)

• 适配兴趣点：直接挑战“记忆即关联”的传统认知，提出Transformer等模型通过几何嵌入空间编码知识，为Agent的“记忆”提供了全新理论基础，尤其适用于理解LLM在复杂任务中的泛化能力。  
。新颖性与价值：首次系统论证几何记忆的涌现性，揭示其与谱偏差的深层联系，为未来构建可解释、可编辑的记忆系统提供理论蓝图。  
。难度与阅读建议：理论性较强，建议先读摘要与结论，重点关注图示与对比实验。  
。可复现性：未知

2. Parallel Unlearning in Inherited Model Networks

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/0ae9f6223c47f3a6833a1807398a117ca3d8a0e0a399f1312f07a123ba54f01f.jpg)

- 适配兴趣点：为“记忆的可控删除”提供了高效、可扩展的框架。通过FIM和MFIM实现并行、单次遗忘，是实现Agent“可遗忘性”的关键技术路径。  
。新颖性与价值：提出基于FIM的继承模型网络遗忘框架，计算开销降低  $99\%$  ，是当前最高效的遗忘方法之一，具有工程落地潜力。  
难度与阅读建议：侧重算法设计与系统实现，建议结合代码理解FIM聚合与参数调整机制。  
。可复现性：公开代码（GitHub）

# 3. Learning Geometry: A Framework for Building Adaptive Manifold Models through Metric Optimization

# ★★★★☆ 4/5

• 适配兴趣点：将“记忆”视为可学习的几何结构，通过优化度量张量使模型自适应数据，为Agent构建“动态记忆空间”提供了开创性范式。  
- 新颖性与价值：将广义相对论中的爱因斯坦-希尔伯特作用量引入ML，赋予“数据驱动几何”物理意义，极具启发性。  
难度与阅读建议：数学推导较深，建议重点关注其与传统固定几何模型的对比与优势分析。  
。可复现性：未知

# 4. Sequences of Logits Reveal the Low Rank Structure of Language Models

# ★★★★☆ 4/5

- 适配兴趣点：揭示了LLM内部存在低秩结构，为“记忆”的压缩、重用与高效生成提供了理论基础，是理解模型记忆效率的关键。  
。新颖性与价值：证明了通过线性组合无关提示的输出可生成目标响应，为“记忆的线性操作”提供了实证支持。  
。难度与阅读建议：实验与理论结合紧密，建议关注其“通用抽象”模型的构建与验证。  
。可复现性：未知

# 5. Action-Driven Processes for Continuous-Time Control

# ★★★★☆ 4/5

- 适配兴趣点：将“动作”作为状态转移的核心驱动力，为建模具有长期记忆和动态决策的Agent提供了统一的理论框架。  
- 新颖性与价值：将控制与推理统一于“动作驱动过程”，为实现具有记忆和因果推理能力的Agent提供了新范式。  
。难度与阅读建议：概念抽象，建议结合spiking neural network的案例理解其应用。

相关论文速览  

<table><tr><td>序号</td><td>标题</td><td>相关性1-5</td><td>关键词</td><td>一句话摘要</td></tr><tr><td>1</td><td>Deep sequence models tend to memorize geometrically; it is unclear why</td><td>★★★★★5/5</td><td>几何记忆,Transformer,知识表征,谱偏差,关系编码</td><td>深度序列模型通过全局几何嵌入空间编码知识,而非局部共现,揭示了记忆的涌现性本质。</td></tr><tr><td>2</td><td>Parallel Unlearning in Inherited Model Networks</td><td>★★★★★☆4/5</td><td>并行遗忘,模型继承,Fisher信息矩阵,一击遗忘,计算效率</td><td>基于FIM和MFIM,实现模型继承网络中高效、并行的单次遗忘,计算开销降低99%。</td></tr><tr><td>3</td><td>Learning Geometry: A Framework for Building Adaptive Manifold Models through Metric Optimization</td><td>★★★★★☆4/5</td><td>自适应几何,度量优化,流形学习,爱因斯坦-希尔伯特,数据驱动几何</td><td>通过优化流形上的度量张量,使模型几何结构自适应数据,为动态记忆提供新范式。</td></tr><tr><td>4</td><td>Sequences of Logits Reveal the Low Rank Structure of Language Models</td><td>★★★★★☆4/5</td><td>低秩结构,对数序列,线性组合,模型可解释性,知识重用</td><td>LLM的对数矩阵具有低秩特性,可通过无关提示的线性组合生成目标响应。</td></tr><tr><td>5</td><td>Action-Driven Processes for Continuous-Time Control</td><td>★★★★★☆4/5</td><td>动作驱动,连续时间控制,信息流,动态决策,spiking神经网络</td><td>将动作作为状态转移的触发器,统一了随机过程与强化学习,为记忆-控制融合提供理论基础。</td></tr><tr><td>6</td><td>Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems</td><td>★★★★★☆3/5</td><td>Pass@K,采样多样性,强化学习,问题求解,代理推理</td><td>优化Pass@K性能,鼓励探索多样解,提升Agent在复杂任务中的推理能力。</td></tr><tr><td>7</td><td>Budgeted Multiple-Expert Deferral</td><td>★★☆☆☆
2/5</td><td>预算化,多专家,专家查询,成本控制,机器学习</td><td>通过选择性查询专家,显著降低训练成本,但与记忆问题关联较弱。</td></tr><tr><td>8</td><td>TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting</td><td>★★☆☆☆
3/5</td><td>零样本,线性RNN,合成预训练,时间序列,状态编织</td><td>基于线性RNN的合成预训练模型,实现高效零样本时间序列预测。</td></tr></table>

# 数据与方法

- 数据：主要为合成数据（如随机微分方程、高斯过程）和公开基准（如Gift-Eval，Mountain Car, CIFAR-10, CelebA），部分涉及真实世界数据（如单细胞测序、交通、医疗）。  
- 方法：核心方法包括：基于Fisher信息矩阵的参数调整（FIUn）、低秩矩阵分解、条件线性动态系统（CLDS）、度量张量优化、动作驱动过程、Pass@K策略优化、合成数据生成。模型架构以Transformer、RNN、Autoencoder、GNN为主。  
- 评价指标：遗忘准确率、保留准确率、计算效率、收敛速度、风险边界、覆盖率、错误率、几何一致性等。对于记忆相关研究，关键指标为遗忘率与保留率的平衡。

# 有价值的内容

- 开源代码：Parallel Unlearning in Inherited Model Networks (GitHub)  
- 复现实验细节：Deep sequence models tend to memorize geometrically 提供了丰富的可视化与对比实验，可复现其几何结构分析。  
- 负面或有启发的结果：Learning Geometry 指出，固定几何模型的表达能力有限，动态几何优化是未来方向。  
- 未知：其余论文的代码与复现细节均未明确提及。

# 兴趣映射

- Agent的记忆本质是什么？→Deep sequence models tend to memorize geometrically (5/5): 提出“几何记忆”是核心，超越了传统关联记忆。

- 如何实现Agent的可遗忘性？  $\rightarrow$  Parallel Unlearning in Inherited Model Networks (4/5): 提供基于FIM的高效并行遗忘框架。  
- 如何让记忆结构可学习和自适应？  $\rightarrow$  Learning Geometry (4/5): 通过优化度量张量,实现“数据驱动几何”, 是动态记忆的范式革命。  
- 记忆如何与控制、决策融合？  $\rightarrow$  Action-Driven Processes (4/5)：将动作作为核心驱动力，统一了记忆与控制的理论基础。  
- 记忆的内部结构是否可被利用？  $\rightarrow$  Sequences of Logits Reveal the Low Rank Structure (4/5): 揭示低秩结构，为记忆的压缩与重用提供了实证支持。

# 阅读路线与行动建议

- 今日最佳阅读路径：优先阅读 Geometric Memory（建立理论认知）  $\rightarrow$  Parallel Unlearning（掌握关键技术）  $\rightarrow$  Adaptive Geometry（探索未来方向）。  
- 可做的小实验与复现方向：1. 用低秩结构方法在小模型上验证“线性组合生成”；2. 在简单任务中复现FIM遗忘流程；3. 分析Transformer的注意力权重是否支持几何记忆假说。  
- 后续跟踪的作者/方向/关键词：Shahriar Noroozizadeh (几何记忆), Di Zhang (自适应几何), Xiao Liu (并行遗忘), 以及关键词: Geometric Memory, Adaptive Geometry, Low-Rank Structure, Action-Driven, Explainable Unlearning.

# 1. Parallel Unlearning in Inherited Model Networks

Relevance:

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/4e2b091afdbcf2f34ebeb3e728643ae996f84fcea330165e5fca5a7dd8a43186.jpg)

arXiv ID: 2408.08493

Authors: Xiao Liu, Mingyuan Li, Guangsheng Yu, Lixiang Li, Haipeng Peng, Ren Ping Liu

TLDR: This paper introduces a novel unlearning framework for model inheritance networks, where models are connected through a chronologically ordered Directed Acyclic Graph (DAG) representing their evolutionary relationships. The core contribution is the Fisher Inheritance Unlearning (FIUn) method, which leverages the Fisher Information Matrix (FIM) to identify and adjust parameters critical to unlearning tasks, enabling efficient and parallel unlearning across inherited models. To support multiple simultaneous unlearning requests, the authors propose the Merging-FIM (MFIM) function, which aggregates FIMs from upstream models into a unified matrix, allowing one-shot removal of inherited knowledge. The framework achieves high unlearning accuracy—0% for unlearned labels in single-class tasks and 1.07% in multi-class tasks—while preserving 94.53% and 84.77% accuracy for retained labels, respectively. The method reduces

computational overhead by  $99\%$  compared to existing approaches, demonstrating significant efficiency gains. Code is publicly available on GitHub.

PDF

# 2. Deep sequence models tend to memorize geometrically; it is unclear why

Relevance:

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/8591b8b429aa63de1ffd0c5e737e563f35194bc8a2039bf550948185dd5fd969.jpg)

arXiv ID: 2510.26745

Authors: Shahriar Noroozizadeh, Vaishnavh Nagarajan, Elan Rosenfeld, Sanjiv Kumar

TLDR: This paper challenges the conventional view of memory in deep sequence models—particularly Transformers—by arguing that these models do not merely memorize local co-occurrences of entities, as traditionally assumed. Instead, they develop a geometric representation of knowledge, where relationships between entities (including those that never co-occur during training) are encoded in a global, structured embedding space. The authors demonstrate that this geometric structure emerges naturally during training, even without explicit architectural or optimization incentives for such geometry. They link this phenomenon to spectral bias and draw parallels with Node2Vec, suggesting that the geometric organization of memory is a fundamental property of neural sequence models. The paper advocates for a shift in perspective from associative (lookup-based) memory to geometric memory, with implications for knowledge acquisition, model capacity, and unlearning in agents.

PDF

# 3. Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems

Relevance:

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/7f8dea1cbec5881f1f1d9227c8bb27b5723f0115c22fc50296c66642b0ffe1c5.jpg)

arXiv ID: 2505.15201

Authors: Christian Walder, Deep Karkhanis

TLDR: This paper introduces Pass-at-k Policy Optimization (PKPO), a novel reinforcement learning method that optimizes for pass@k performance—i.e., the probability that at least one of k sampled solutions is correct—rather than the traditional pass@1 metric. By deriving low-variance,

unbiased estimators for pass@k and its gradient in both binary and continuous reward settings, PKPO enables efficient optimization of collective solution quality. The approach transforms final rewards to prioritize joint utility across multiple samples, enhancing exploration and enabling the solution of harder problems. The method supports arbitrary  $k \leq n$  and allows annealing  $k$  during training, achieving improvements in both pass@1 and pass@k performance. Experiments on toy tasks and with the GEMMA-2 LLM demonstrate that PKPO improves problem-solving on challenging tasks by encouraging diverse and high-quality solution sets, particularly where standard RL stalls. This suggests a promising direction for improving agent reasoning through better utilization of multiple sampling attempts.

PDF

# 4. Sequences of Logits Reveal the Low Rank Structure of Language Models

Relevance:

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/d1be2fb8406c2db9180b80da2589dd1beddc6260c716b3b84ef9c7d93b881028.jpg)

arXiv ID: 2510.24966

Authors: Noah Golowich, Allen Liu, Abhishek Shetty

TLDR: This paper investigates the low-rank structure in large language models by analyzing sequences of logits across different prompts and responses. The authors demonstrate empirically that the logits matrices generated by diverse language models exhibit low approximate rank, suggesting an underlying low-dimensional representation. They further show that this structure can be exploited for text generation—specifically, generating responses to a target prompt by taking linear combinations of outputs from unrelated or nonsensical prompts. Theoretical analysis supports these findings with a universal abstraction that captures the model's behavior, accompanied by provable learning guarantees and representation power analysis. The work offers a model-agnostic perspective on understanding and leveraging the internal structure of language models.

PDF

# 5. Action-Driven Processes for Continuous-Time Control

Relevance:

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/4f686eeae326eded9599fb29be37baa7c47b59615d8aee34babbd3a41aaa9150.jpg)

arXiv ID: 2510.26672

Authors: Ruimin He, Shaowei Lin

TLDR: This paper introduces the concept of action-driven processes, which unify stochastic processes and reinforcement learning by treating actions as fundamental triggers for state transitions. The framework is grounded in control-as-inference principles, showing that minimizing the Kullback-Leibler divergence between a policy-driven true distribution and a reward-driven model distribution for an action-driven process is equivalent to maximum entropy reinforcement learning. The approach is applied to spiking neural networks, highlighting its potential for modeling continuous-time control systems with memory and dynamic decision-making. The work emphasizes the role of actions in shaping information flow and system dynamics, offering a theoretical foundation for integrating memory and control in intelligent agents.

PDF

# 6. Reward Collapse in Aligning Large Language Models

Relevance:

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/b92777da6dac1a4567a592481786871574581ce7b52308ad46dd836282b55b8a.jpg)

arXiv ID: 2305.17608

Authors: Ziang Song, Tianle Cai, Jason D. Lee, Weijie J. Su

TLDR: This paper investigates the phenomenon of 'reward collapse' in the alignment of large language models (LLMs), where ranking-based reward models converge to an identical reward distribution across different prompts during the final training phase, despite the need for prompt-specific reward variation. The authors identify that the root cause lies in the ranking-based objective's inability to incorporate prompt-specific information during optimization. They provide a theoretical analysis that derives closed-form expressions for reward distributions in an asymptotic regime and propose a prompt-aware optimization scheme to ensure prompt-dependent rewards. Experiments demonstrate that their method effectively mitigates reward collapse, improving the expressiveness and utility of reward models for diverse prompts.

PDF

# 7. Differentiable Programming for Differential Equations: A Review

Relevance:

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/2f6073a70d4d9bb813ac84c88c35005abf31f8bc95ac1826c25a1812c45574ae.jpg)

arXiv ID: 2406.09699

Authors: Facundo Sapienza, Jordi Bolibar, Frank Sch\*afer, Brian Groenke, Avik Pal, Victor

Boussange, Patrick Heimbach, Giles Hooker, Fernando P'erez, Per-Olof Persson, Christopher Rackauckas

TLDR: This paper provides a comprehensive review of differentiable programming techniques for computing gradients of numerical solutions to differential equations. It highlights the importance of such gradients in scientific computing, particularly in sensitivity analysis, parameter inversion, and training hybrid models that combine differential equations with data-driven methods. The review covers mathematical foundations of various approaches—such as adjoint methods, forward-mode differentiation, and automatic differentiation—compares their strengths and limitations, discusses computational considerations, and presents best practices for implementation using modern scientific software. The work emphasizes the synergy between inverse problems and machine learning, aiming to unify scientific modeling with data-driven techniques.

PDF

# 8. Modeling Neural Activity with Conditionally Linear Dynamical Systems

Relevance:

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/ea96679fc4e87bfd9587b5bbcaa42c3d42de678ac923a52facd7d7fbca0ab374.jpg)

arXiv ID: 2502.18347

Authors: Victor Geadah, Amin Nejatabkhsh, David Lipshutz, Jonathan W. Pillow, Alex H. Williams

TLDR: This paper introduces Conditionally Linear Dynamical Systems (CLDS), a modeling framework that combines Gaussian Process priors with linear dynamical systems to capture complex, nonlinear neural population dynamics. The model conditions linear dynamics on task and behavioral variables via Gaussian Processes, enabling interpretable and tractable Bayesian inference. CLDS is particularly effective in data-scarce scenarios (e.g., one trial per condition) due to its ability to share statistical strength across similar task conditions. The authors demonstrate its utility in modeling thalamic neurons encoding heading direction and motor cortical neurons during a cued reaching task, showing strong performance in capturing condition-dependent neural dynamics.

PDF

# 9. TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting

Relevance:

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/b7cba60828db109cc03d4fb5217d91063b598203c50b1199bb22f104bc4bc95f.jpg)

arXiv ID: 2510.25502

Authors: Vladyslav Moroshan, Julien Siems, Arber Zela, Timur Carstensen, Frank Hutter

TLDR: This paper introduces TempoPFN, a univariate time series foundation model based on linear Recurrent Neural Networks (RNNs) that is pre-trained exclusively on synthetic data for zero-shot time series forecasting. The model employs a GatedDeltaProduct architecture with state-weaving, enabling fully parallelizable training and inference across sequence lengths without requiring windowing or summarization. A comprehensive synthetic data pipeline integrates diverse generators such as stochastic differential equations, Gaussian processes, and audio synthesis, along with novel augmentations. On the Gift-Eval benchmark, TempoPFN achieves top-tier performance, outperforming other synthetic-only models and most real-data-trained models, while offering greater efficiency. The authors release their data generation pipeline and training code to support reproducibility and future research.

PDF

# 10. Conformal Prediction Beyond the Horizon: Distribution-Free Inference for Policy Evaluation

Relevance: ★★★★

arXiv ID: 2510.26026

Authors: Feichen Gan, Youcun Lu, Yingying Zhang, Yukun Liu

TLDR: This paper presents a conformal prediction framework for reliable uncertainty quantification in reinforcement learning (RL), specifically for infinite-horizon policy evaluation. It introduces a distribution-free method to construct prediction intervals for returns in both on-policy and off-policy settings. The approach combines distributional RL with conformal calibration, using a modular pseudo-return construction based on truncated rollouts and a time-aware calibration strategy involving experience replay and weighted subsampling. These techniques help address challenges like unobserved returns, temporal dependencies, and distributional shifts, while maintaining coverage guarantees even under model misspecification and importance weight estimation errors. The method is validated on synthetic and benchmark environments such as Mountain Car, demonstrating improved coverage and reliability compared to standard distributional RL baselines.

PDF

# 11. LLMs as In-Context Meta-Learners for Model and Hyperparameter Selection

Relevance:

arXiv ID: 2510.26510

Authors: Youssef Attia El Hili, Albert Thomas, Malik Tiomoko, Abdelhakim Benechehab, Corentin L'eger, Corinne Ancourt, Bal'azs K'égl

TLDR: This paper explores the use of large language models (LLMs) as in-context meta-learners for model and hyperparameter selection in machine learning. By transforming datasets into interpretable metadata, the authors prompt LLMs to recommend suitable model families and hyperparameters. Two prompting strategies are evaluated: zero-shot (relying on pretrained knowledge) and meta-informed (using past task examples). Experiments on synthetic and real-world benchmarks show that LLMs can effectively recommend competitive configurations without extensive search, with the meta-informed approach demonstrating clear improvements—indicating their capacity for in-context meta-learning. The study positions LLMs as lightweight, general-purpose tools for automated model selection and hyperparameter tuning.

PDF

# 12. Budgeted Multiple-Expert Deferral

Relevance:

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/bbeca605225619a8cc18e6a2b02916ab66c8fd649530a06a3e6d6a997672c975.jpg)

arXiv ID: 2510.26706

Authors: Giulia DeSalvo, Clara Mohri, Mehryar Mohri, Yutao Zhong

TLDR: This paper introduces a budgeted deferral framework for training multiple-expert machine learning systems in a cost-effective manner. Traditional deferral methods require querying all experts during training, which is expensive when experts are resource-intensive. The proposed approach selectively queries only a subset of experts per training instance, reducing overall cost while maintaining predictive performance. The authors develop both two-stage and single-stage algorithms with theoretical guarantees, including generalization bounds and label complexity analyses. Empirical results across multiple domains demonstrate significant reductions in training costs without compromising accuracy, making the method practical for real-world applications where expert queries are costly.

PDF

# 13. Learning Geometry: A Framework for Building Adaptive Manifold Models through Metric Optimization

Relevance: ★★★

arXiv ID: 2510.26068

Authors: Di Zhang

TLDR: This paper introduces a novel machine learning framework that treats the model's geometric structure as a dynamic entity to be optimized. Instead of fixing the geometry of the model space, the authors propose optimizing the metric tensor field on a manifold with a predefined topology, allowing the model's intrinsic geometry to adapt to data. The framework uses a variational loss function that balances data fidelity and geometric complexity, with the latter acting as a regularizer to prevent overfitting. To make the infinite-dimensional optimization tractable, the authors discretize the manifold into a triangular mesh and parameterize the metric via edge lengths, enabling efficient optimization with automatic differentiation. Theoretical insights reveal a deep connection to the Einstein-Hilbert action in general relativity, offering a physical interpretation of 'data-driven geometry'. The paper argues that metric optimization with fixed topology provides greater expressive power than fixed-geometry models, paving the way for future 'meta-learners' that can autonomously evolve their geometry and topology. Applications include scientific model discovery and robust representation learning.

PDF

# 14. Neural active manifolds: nonlinear dimensionality reduction for uncertainty quantification

Relevance:

arXiv ID: 2408.03534

Authors: Andrea Zanoni, Gianluca Geraci, Matteo Salvador, Alison L. Marsden, Daniele E. Schiavazzi

TLDR: This paper introduces Neural Active Manifolds (NeurAM), a novel nonlinear dimensionality reduction method tailored for computationally expensive mathematical models. By using autoencoders, the approach identifies a one-dimensional manifold that captures the variability in model outputs, aided by a surrogate model trained on this manifold. The method operates without requiring gradient information, relying solely on model evaluations. It enables efficient sampling for tasks like sensitivity analysis and uncertainty quantification, particularly in multifidelity settings. Theoretical and numerical results demonstrate that sampling on the discovered low-dimensional manifold reduces variance in estimators, improving efficiency. The framework is validated on challenging test cases, showing advantages over existing methods.

# 15. Meta-Learning Objectives for Preference Optimization

Relevance:

arXiv ID: 2411.06568

Authors: Carlo Alfano, Silvia Sapora, Jakob Nicolaus Foerster, Patrick Rebeschini, Yee Whye Teh

TLDR: This paper introduces a diagnostic framework using MuJoCo tasks to evaluate preference optimization (PO) algorithms for large language models (LLMs) in a more controlled and cost-effective manner. The authors propose a novel family of PO algorithms based on mirror descent, called Mirror Preference Optimization (MPO), and use evolutionary strategies to discover specialized algorithms tailored to specific data characteristics such as noise or mixed quality. The discovered algorithms outperform existing methods in MuJoCo benchmarks and, leveraging insights from these experiments, they design a PO algorithm that significantly improves performance in LLM alignment tasks.

PDF

# 16. The Neural Pruning Law Hypothesis

Relevance: ★★★

arXiv ID: 2504.05349

Authors: Eugen Barbulescu, Antonio Alexoaie, Lucian Busoniu

TLDR: This paper introduces Hyperflux, a conceptually grounded framework for neural network pruning that models the pruning process through the interaction of weight flux (the gradient's response to weight removal) and network pressure (a global regularization pushing weights toward pruning). The authors discover a power-law relationship between the minimum weight flux and network density, which they generalize into the 'Neural Pruning Law Hypothesis'—the claim that this power-law holds across various saliency metrics and pruning methods (e.g., magnitude, gradient-based,  $\$ 1\_0$ ). The hypothesis is validated across multiple pruning techniques, suggesting a unifying principle in neural network pruning.

PDF

# 17. Learning with Calibration: Exploring Test-Time Computing of Spatio-Temporal Forecasting

Relevance: ★★★

arXiv ID: 2506.00635

Authors: Wei Chen, Yuxuan Liang

TLDR: This paper introduces ST-TTC (Spatio-Temporal Test-Time Computing), a novel test-time computing framework for spatio-temporal forecasting that addresses challenges like signal anomalies, noise, and distributional shifts. The method employs a spectral-domain calibrator with phase-amplitude modulation to correct periodic structural biases caused by non-stationarity during testing. It also features a flash updating mechanism with a streaming memory queue to enable efficient real-time bias correction without requiring complex training-stage modifications. The approach is computationally efficient, generalizable, and effective across multiple real-world datasets, demonstrating improved accuracy and adaptability in dynamic environments.

PDF

# 18. Purifying Shampoo: Investigating Shampoo's Heuristics by Decomposing its Preconditioner

Relevance:

arXiv ID: 2506.03595

Authors: Runa Eschenhagen, Aaron Defazio, Tsung-Hsien Lee, Richard E. Turner, Hao-Jun Michael Shi

TLDR: This paper investigates the heuristics underlying the Shampoo optimization algorithm, a Kronecker-factorization-based method that has shown success in large-scale neural network training. The authors analyze key heuristics such as learning rate grafting and stale preconditioning, which contribute to Shampoo's performance but also increase complexity and require manual tuning. By decomposing the preconditioner and examining its eigenvalues and eigenbasis updates, the paper demonstrates that grafting from Adam helps correct issues related to eigenvalue staleness and mis-scaling. Furthermore, the authors show that directly correcting eigenvalues can eliminate the need for learning rate grafting. To address the error from infrequent eigenbasis updates, they propose an adaptive criterion inspired by warm-started QR algorithms, which allows for decoupled update frequencies and enables systematic study of approximation error effects. The work provides a principled pathway to simplify and improve Shampoo-like algorithms by reducing reliance on ad-hoc heuristics.

# 19. Linearized Optimal Transport for Analysis of High-Dimensional Point-Cloud and Single-Cell Data

Relevance:

arXiv ID: 2510.22033

Authors: Tianxiang Wang, Yingtong Ke, Dhananjay Bhaskar, Smita Krishnaswamy, Alexander Cloninger

TLDR: This paper introduces Linear Optimal Transport (LOT) as a framework to embed high-dimensional, irregular point clouds—such as those from single-cell data—into a fixed-dimensional Euclidean space while preserving distributional and geometric relationships. The method enables interpretable, linear representations of complex biological data, allowing for accurate classification of patient states (e.g., COVID-19 severity) with classifier weights traceable to specific cellular markers and spatial regions. It also supports synthetic data generation via LOT barycenters, which can represent averaged cellular profiles for applications like drug interaction testing. The approach combines predictive power, biological interpretability, and generative modeling, offering a transparent alternative to black-box nonlinear methods.

# 20. The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity

Relevance: ★★★

arXiv ID: 2510.23965

Authors: Ali Aouad, Aymane El Gadarri, Vivek F. Farias

TLDR: This paper introduces the sign estimator, a novel method for aligning large language models (LLMs) that addresses the challenge of heterogeneity in human preferences. Traditional alignment approaches, such as those based on pairwise comparisons, often produce inconsistent estimates of population-average utility due to varying individual preferences. The sign estimator replaces cross-entropy loss with binary classification loss in the aggregation step, resulting in a provably consistent and efficient estimator under mild assumptions. It achieves the first polynomial finite-sample error bounds in this context and demonstrates significant improvements in realistic

simulations using digital twins—reducing angular estimation error by nearly  $35\%$  and lowering disagreement with true population preferences from  $12\%$  to  $8\%$  compared to standard RLHF. Importantly, it outperforms existing panel data methods that model individual heterogeneity while retaining the simplicity of standard alignment pipelines.

# PDF

# 21. Using latent representations to link disjoint longitudinal data for mixed-effects regression

Relevance:

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/f0fa460edcf625c0f08256218ddc99994c67c3db49967d1e439462fef326e8ea.jpg)

arXiv ID: 2510.25531

Authors: Clemens Sch\''achter, Maren Hackenberg, Michelle Pfaffenlehner, F'elix B. Tamben-Donfack, Thorsten Schmidt, Astrid Pechmann, Janbernd Kirschner, Jan Hasenauser, Harald Binder

TLDR: This paper addresses the challenge of analyzing disjoint longitudinal data in rare disease studies, where measurement instruments change over time (e.g., due to age-specific adaptations), making traditional mixed-effects regression difficult. To overcome this, the authors propose using variational autoencoders (VAEs) to map observations from different instruments into a shared low-dimensional latent space, aligning them temporally. This enables longitudinal modeling across instruments by applying mixed-effects regression on the latent representations. The method includes a novel statistical testing framework that accounts for the joint estimation of VAEs and regression models, supporting valid inference. The approach is applied to spinal muscular atrophy patients to assess treatment switch effects, successfully mapping results back to the original item level. The framework supports model selection and effect quantification, offering a promising solution for small-sample, heterogeneous longitudinal data.

# PDF

# 22. E-Scores for (In)Correctness Assessment of Generative Model Outputs

Relevance:

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/56e8bdf038faac54d7c90f5d22c20cc310530cdecab9d92109a30580b9c50463.jpg)

arXiv ID: 2510.25770

Authors: Guneet S. Dhillon, Javier Gonz\{'alez, Teodora Pandeva, Alicia Curth

TLDR: This paper introduces e-scores, a novel method for assessing the correctness of generative model outputs—particularly large language models (LLMs)—using e-values within the conformal prediction framework. Unlike traditional p-value-based approaches that are vulnerable to p-hacking (e.g., post-hoc selection of tolerance levels undermining guarantees), e-scores allow for adaptive, post-hoc calibration of error tolerance while maintaining statistical validity. The authors demonstrate that e-scores effectively bound a post-hoc error measure known as size distortion, ensuring robustness in correctness assessment. The method is empirically validated on two correctness types: mathematical factuality and satisfaction of property constraints, showing strong performance and flexibility.

PDF

# 23. The Ray Tracing Sampler: Bayesian Sampling of Neural Networks for Everyone

Relevance: ★★★

arXiv ID: 2510.25824

Authors: Peter Behroozzi

TLDR: This paper introduces a novel Bayesian sampling method called the Ray Tracing Sampler, which uses ray propagation in a parameter space with a refractive index determined by the likelihood function. The method is designed to efficiently sample posterior distributions of neural network parameters, offering significant improvements in resilience to gradient noise and the ability to traverse complex likelihood landscapes, including regions with likelihood barriers or holes. The approach is shown to be highly scalable, enabling posterior sampling for large models like GPT-2 (1.5 billion parameters) on a single consumer GPU. The framework unifies several existing sampling techniques—such as HMC, Metropolis, and Gibbs—under a generalized ray tracing perspective, allowing sampling according to arbitrary weighting functions. The method is implemented in C, JAX, and PyTorch with public code available.

PDF

# 24. Contrastive Predictive Coding Done Right for Mutual Information Estimation

Relevance: ★★★

arXiv ID: 2510.25983

Authors: J. Jon Ryu, Pavan Yeddanapudi, Xiangxiang Xu, Gregory W. Wornell

TLDR: This paper critically examines the InfoNCE objective, widely used for mutual information (MI) estimation in contrastive learning, and argues that it is not a valid MI estimator due to its indirect connection to MI. The authors propose a modified version called InfoNCE-anchor, which introduces an auxiliary anchor class to enable consistent density ratio estimation and reduce bias in MI estimation. They further generalize their approach using proper scoring rules, which unify various contrastive objectives (e.g., NCE, InfoNCE, f-divergence variants) under a single principled framework. Empirical results show that InfoNCE-anchor with the log score provides the most accurate MI estimates, but it does not improve downstream task performance in self-supervised representation learning. The authors conclude that the success of contrastive learning stems from learning structured density ratios rather than accurate MI estimation per se.

PDF

# 25. Uncertainty-Aware Diagnostics for Physics-Informed Machine Learning

Relevance:

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/394c1ba6642c1390d4a37bc10e75d1799e8fae4b688f361715ef97af0c632ce4.jpg)

arXiv ID: 2510.26121

Authors: Mara Daniels, Liam Hodgkinson, Michael Mahoney

TLDR: This paper introduces the Physics-Informed Log Evidence (PILE) score, a novel uncertainty-aware metric for evaluating and selecting hyperparameters in physics-informed machine learning (PIML) models. The PILE score operates within a Gaussian process regression framework and addresses the challenge of ambiguous model quality assessment in multi-objective PIML settings —where both data fidelity and physical constraints (e.g., differential equations) are optimized. By providing a single, principled metric that accounts for epistemic uncertainty, PILE enables better hyperparameter selection, including kernel bandwidth, regularization weights, and kernel function choice. The method also supports a 'data-free' scenario, allowing a priori identification of kernel functions well-suited to specific PDEs. The authors suggest that PILE can be extended beyond kernel-based models to broader PIML architectures.

PDF

# 26. Cyclic Counterfactuals under Shift-Scale Interventions

Relevance:

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/df694d950fe776fcde7793412864afa0ddca6cbc1f4d9dd8d245ee76c2ea80de.jpg)

arXiv ID: 2510.25005

Authors: Saptarshi Saha, Dhruv Vansraj Rathore, Utpal Garain

TLDR: This paper addresses counterfactual inference in cyclic structural causal models (SCMs), which allow for feedback loops and cyclic dependencies—common in real-world systems like biological networks. Traditional counterfactual frameworks rely on acyclic SCMs (DAGs), but this work extends the theory to handle shift-scale interventions (soft, policy-like changes that rescale or shift a variable's mechanism) in cyclic settings. The authors develop a formal framework for computing counterfactuals under such interventions, providing a way to reason about causal effects in systems with circular dependencies, which is a significant advancement for modeling complex, dynamic systems.

PDF

# 27. Doubly Robust Alignment for Large Language Models

Relevance:

arXiv ID: 2506.01183

Authors: Erhan Xu, Kai Ye, Hongyi Zhou, Luhan Zhu, Francesco Quinzan, Chengchun Shi

TLDR: This paper proposes a doubly robust preference optimization (DRPO) algorithm for reinforcement learning from human feedback (RLHF) in large language models. The method is designed to be robust against model misspecification in either the preference model (e.g., Bradley-Terry model) or the reference policy, ensuring consistent performance when at least one of them is correctly specified. This improves the stability and reliability of model alignment compared to existing RLHF approaches. Theoretical analysis and empirical results demonstrate superior performance across various benchmarks, with code publicly available.

PDF

# 28. Cybersecurity threat detection based on a UEBA framework using Deep Autoencoders

Relevance:

arXiv ID: 2505.11542

Authors: Jose Fuentes, Ines Ortega-Fernandez, Nora M. Villanueva, Marta Sestelo

TLDR: This paper presents an explainable User and Entity Behavior Analytics (UEBA) framework for cybersecurity threat detection using Deep Autoencoders combined with Doc2Vec to handle both numerical and textual data. The approach builds normal behavior profiles to detect anomalies, with a focus on providing interpretable results that help trace the origin of detected threats. The authors also provide a theoretical proof on the equivalence of two definitions of fully-connected neural networks. Experimental results show the framework effectively detects both real and synthetic anomalies from attack data, demonstrating strong performance in identifying security incidents while offering explainability, making it suitable for integration into enterprise security systems.

PDF

# 29. Distributed optimization: designed for federated learning

Relevance

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/b658cf69b777cbda3e8867324aca6c826700423b52e8751335a97946bbfb9c64.jpg)

arXiv ID: 2508.08606

Authors: Wenyou Guo, Ting Qu, Chunrong Pan, George Q. Huang

TLDR: This paper presents a class of distributed optimization algorithms based on the augmented Lagrangian method, specifically designed for federated learning (FL) in both centralized and decentralized settings with diverse communication topologies. The approach incorporates proximal relaxation and quadratic approximation to generalize classical optimization methods such as gradient descent and stochastic gradient descent. The framework includes multiple termination criteria and parameter update mechanisms to improve computational efficiency, with strong theoretical convergence guarantees. Numerical experiments show effective performance in large-scale, statistically heterogeneous FL environments.

PDF

# 30. Statistical Inference for Matching Decisions via Matrix Completion under Dependent Missingness

Relevance:

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/0071b7498e4b95c5368dd116d5ce681dd68d2e27a94a7039217f5dddf6627777.jpg)

arXiv ID: 2510.26478

Authors: Congyuan Duan, Wanteng Ma, Dong Xia, Kan Xu

TLDR: This paper addresses statistical inference in two-sided matching markets using matrix

completion, focusing on the challenge of dependent missingness caused by matching capacity constraints. Unlike traditional matrix completion that assumes independent sampling, this work accounts for the dependence induced by matching mechanisms. The authors propose a nonconvex Grassmannian gradient descent algorithm and establish near-optimal entrywise convergence rates for three matching scenarios: one-to-one, one-to-many with one-sided random arrival, and two-sided random arrival. They further develop a debiasing and projection framework to enable valid uncertainty quantification and hypothesis testing for linear forms of the reward matrix, achieving asymptotic normality with finite-sample guarantees. Empirical results show accurate estimation, valid confidence intervals, and effective policy evaluation.

PDF

# 31. On Measuring Localization of Shortcuts in Deep Networks

Relevance:

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/80d444d25efa4743d195a7ab03e6d61e17528aa56c433b89bd1dfea0686bdfc9.jpg)

arXiv ID: 2510.26560

Authors: Nikita Tsoy, Nikola Konstantinov

TLDR: This paper investigates the layer-wise localization of shortcuts—spurious patterns that improve training performance but harm generalization—in deep neural networks. Using a counterfactual training approach on clean and skewed datasets, the authors quantify how different layers contribute to accuracy degradation caused by shortcuts. Experiments across multiple architectures (VGG, ResNet, DeiT, ConvNeXt) and datasets (CIFAR-10, Waterbirds, CelebA) reveal that shortcut learning is not confined to specific layers but is distributed across the network. Shallow layers tend to encode spurious features, while deeper layers tend to forget meaningful, core features present in clean data. The study also identifies key axes of variation in shortcut localization and suggests that effective mitigation strategies may need to be tailored to specific datasets and architectures rather than being universally applicable.

PDF

# 32. Wasserstein Rate Driven CLTs for Markov Chains with Weighted Lipschitz, Sobolev, and Stein Test Functions

Relevance:

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/468273e8586082849ebbb3cdad672fe5ed7c3fb46357e87a23c62277ff2f83b2.jpg)

arXiv ID: 2002.09427

Authors: Rui Jin, Aixin Tan

TLDR: This paper focuses on establishing central limit theorems (CLTs) for Markov chains based on convergence rates in Wasserstein distance, which is more scalable in high-dimensional settings compared to total variation (TV) distance. The authors derive two CLTs that directly use sub-geometric convergence rates in Wasserstein distance for Lipschitz functions under moment conditions, and propose methods to extend these results to broader function classes such as weighted Lipschitz, weighted Sobolev, and Stein test functions. The analysis leverages coupling arguments and analytic techniques, particularly for  $\$W\_2$ convergence. The theoretical results are applied to five examples, including nonlinear autoregressive processes, ULA, EI-MALA, and reducible autoregressive models. The work provides a robust framework for error analysis in Monte Carlo methods using Wasserstein-based convergence guarantees.

PDF

# 33. A Fourier-based inference method for learning interaction kernels in particle systems

Relevance:

arXiv ID: 2505.05207

Authors: Grigorios A. Pavliotis, Andrea Zanoni

TLDR: This paper proposes a Fourier-based inference method for learning interaction kernels in stochastic interacting particle systems from observations of a single particle. It employs a semi-parametric approach, representing the interaction kernel via a generalized Fourier series with basis functions derived from orthogonal polynomials relative to the invariant measure of the mean-field dynamics. The Fourier coefficients are estimated by solving a linear system whose coefficients depend on moments of the invariant measure, approximated from observed particle trajectories. The study analyzes the approximation error in a weighted Lebesgue space and investigates the estimator's asymptotic properties in the joint large time-mean field limit. The paper also examines scenarios requiring increasing numbers of Fourier coefficients for accurate kernel representation, supported by extensive numerical simulations.

PDF

# 34. Direct Debiased Machine Learning via Bregman Divergence Minimization

Relevance:

arXiv ID: 2510.23534

Authors: Masahiro Kato

TLDR: This paper introduces a direct debiased machine learning framework that unifies several key methods in causal inference and statistical estimation, including Riesz regression, covariate balancing, targeted maximum likelihood estimation (TMLE), and density-ratio estimation. The core idea is to minimize the Bregman divergence between Neyman orthogonal scores computed with known and unknown nuisance parameters—this process, termed Neyman targeted estimation, enables end-to-end estimation of both the regression function and the Riesz representative. The framework is flexible, as different Bregman divergences (e.g., squared loss, Kullback-Leibler divergence) correspond to different estimation techniques, such as Riesz regression or entropy balancing. Notably, it automatically achieves covariate balancing in certain model settings and generalizes TMLE. The method aims to reduce bias from first-stage machine learning estimates in causal and structural models by leveraging Neyman orthogonality.

PDF

# 35. Sequential Change Detection Under A Markov Setup With Unknown Pre-Change and Post-Change Distributions

Relevance:

arXiv ID: 2510.26204

Authors: Ashish Bhoopesh Gulaguli, Shashwat Singh, Rakesh Kumar Bansal

TLDR: This paper extends a sequential change detection algorithm originally developed for independent and identically distributed (i.i.d.) data to a Markov setup. The method employs Page's CUSUM statistic, uses the empirical distribution to estimate the pre-change distribution, and applies a universal code to estimate the post-change distribution. The key contribution is adapting the change detection framework to handle dependencies in data governed by Markov processes, which is particularly useful in scenarios where data exhibit temporal correlations. The theoretical analysis ensures the algorithm maintains desirable detection performance under the more complex Markov assumption.

PDF

# 36. Posterior Sampling by Combining Diffusion Models with Annealed Langevin Dynamics

Relevance:

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/b5b08c169051fea908182defc49150892baf5c812ca7ba224f064f330b456579.jpg)

arXiv ID: 2510.26324

Authors: Zhiyang Xun, Shivam Gupta, Eric Price

TLDR: This paper addresses the problem of posterior sampling in the context of noisy linear measurements, specifically when the prior distribution  $p(x)$  is well-approximated. The focus is on log-concave distributions, where traditional Langevin dynamics struggles due to sensitivity to score estimation errors. The authors propose a novel method that combines diffusion models with annealed Langevin dynamics to achieve efficient and accurate posterior sampling under a weaker  $\$ 1^{\wedge}4$  bound on score error, which is a significant improvement over prior methods requiring stricter sub-exponential or MGF bounds. This approach enables polynomial-time conditional sampling and is applicable to tasks like image inpainting, deblurring, and MRI reconstruction.

PDF

# 37. Higher-Order Regularization Learning on Hypergraphs

Relevance:

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/9384dd941ab563f720a057136694ae153888acef1e5fee95a9d27174db524949.jpg)

arXiv ID: 2510.26533

Authors: Adrien Weihs, Andrea Bertozzi, Matthew Thorpe

TLDR: This paper introduces Higher-Order Hypergraph Learning (HOHL), a method that enforces higher-order smoothness in data through powers of multiscale Laplacians derived from hypergraph structures. It extends prior theoretical work by proving the consistency of a truncated version of HOHL and deriving explicit convergence rates when used as a regularizer in supervised learning. The method demonstrates strong empirical performance in active learning and on datasets without clear geometric structure, showcasing its robustness and versatility across diverse settings.

PDF

# 38. Bridging the Gap between Empirical Welfare Maximization and Conditional Average Treatment Effect Estimation in Policy Learning

Relevance:

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/2f4114659687d94ec1c75d03eb5df84008568d53dc14d8154ae4b3cc354329eb.jpg)

arXiv ID: 2510.26723

Authors: Masahiro Kato

TLDR: This paper establishes an exact equivalence between two major approaches in policy learning: Empirical Welfare Maximization (EWM) and the plug-in approach based on Conditional Average Treatment Effect (CATE) estimation. It shows that both methods stem from the same underlying optimization problem when the policy class is appropriately reparameterized. This equivalence allows for interchangeable use of the two methods and shares the same theoretical guarantees under standard conditions. The authors leverage this insight to propose a novel regularization method that leads to a convex, computationally efficient training procedure, avoiding the typically NP-hard combinatorial steps in EWM. The work contributes a unified framework for policy learning with practical improvements in efficiency and scalability.

PDF

# 39. Infinite-dimensional Mahalanobis Distance with Applications to Kernelized Novelty Detection

Relevance:

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/908d3f6fe01bc7f1627afc5edd97eed1de6f1248b12937eab50618f1e8d26fab.jpg)

arXiv ID: 2407.11873

Authors: Nikita Zozoulenko, Thomas Cass, Lukas Gonon

TLDR: This paper extends the classical Mahalanobis distance to infinite-dimensional separable Banach spaces by reinterpreting it as a Cameron-Martin norm associated with a probability measure. The proposed variance norm is basis-free and data-driven, enabling anomaly detection in high-dimensional and functional data settings, including non-injective covariance operators. The framework generalizes classical and kernelized Mahalanobis distances, with theoretical guarantees on invariance under invertible bounded linear transformations and consistency under Tikhonov regularization. The authors introduce a kernelized nearest-neighbour Mahalanobis distance and demonstrate its superior performance over traditional methods in multivariate time series novelty detection across 12 real-world datasets using advanced kernels like signature, global alignment, and Volterra reservoir kernels.

PDF

# 40. End-to-end guarantees for indirect data-driven control of bilinear systems with finite stochastic data

Authors: Nicolas Chatzikiriakos, Robin Str\"asser, Frank Allg\"ower, Andrea Iannelli

TLDR: This paper presents an end-to-end data-driven control method for bilinear systems using finite stochastic data with probabilistic noise. It introduces a novel approach to identify bilinear system matrices by transforming the problem into a series of linear and affine identification tasks through a carefully designed control input during data collection. The authors derive both a priori and data-dependent finite-sample identification error bounds, including ellipsoidal bounds that are suitable for control design. These error bounds are then integrated into a robust controller design that ensures exponential stability of the closed-loop system. The method is validated through extensive numerical experiments, and connections are drawn to Koopman operator-based control of general nonlinear systems, suggesting broader applicability.

PDF

# 41. Stability and Sharper Risk Bounds with Convergence Rate  $\mathbb{S}$ tilde{O}  $(1 / n^{\wedge}2)$

Relevance:

arXiv ID: 2410.09766

Authors: Bowei Zhu, Shaojie Li, Mingyang Yi, Yong Liu

TLDR: This paper improves the excess risk bounds for strongly-convex learners using algorithmic stability, achieving a convergence rate of  $\$ \backslash$ tilde{O}  $(1 / n^2)$  under the Polyak-Lojasiewicz condition, smoothness, and Lipschitz continuity of losses. The authors demonstrate that this rate is tight and represents the best-known high-probability generalization bounds for gradient-based methods in nonconvex settings, surpassing previous  $\$ 0(\backslash \log (n) / n)$  bounds. The work contributes to theoretical understanding of generalization in optimization-based learning, particularly in scenarios involving iterative algorithms and stability analysis.

PDF

# 42. Beyond likelihood ratio bias: Nested multi-time-scale stochastic approximation for likelihood-free parameter estimation

# Relevance:

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/a82e717c7c278dbc130f4cc96581d488822263a8e7a121db5aca3d7c2adbbd43.jpg)

arXiv ID: 2411.12995

Authors: Zehao Li, Zhouchen Lin, Yijie Peng

TLDR: This paper addresses parameter inference in simulation-based stochastic models where the likelihood function is unknown. The authors propose a ratio-free nested multi-time-scale stochastic approximation (NMTS) method that simultaneously estimates the score and updates model parameters, avoiding the bias and instability caused by likelihood ratio estimation. The method achieves strong convergence, asymptotic normality, and improved convergence rates compared to existing approaches. By carefully choosing step sizes, the algorithm can reach optimal convergence rates in multi-time-scale stochastic approximation. Numerical results show significant improvements in estimation accuracy—up to two orders of magnitude—without increasing computational cost, making it efficient for complex stochastic systems.

# PDF

# 43. On the Impact of Performative Risk Minimization for Binary Random Variables

Relevance:

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/deb32a65e51f3fbc541180f080d3619c15715ff30dfe650f73950ceb9a7826d9.jpg)

arXiv ID: 2502.02331

Authors: Nikita Tsoy, Ivan Kirev, Negin Rahimiayazdi, Nikola Konstantinov

TLDR: This paper investigates performative risk minimization (PRM), a framework designed to maintain machine learning model accuracy in the face of distribution shifts caused by performativity—where predictions influence the very outcomes they aim to predict. The study focuses on binary random variables and linear performative shifts, analyzing the impact of PRM through two proposed impact measures. Under full information, the authors derive explicit solutions and impact metrics. For partial information scenarios, they propose performative-aware statistical estimators and validate them through simulations. The results show that PRM can lead to amplified side effects compared to non-performative methods, highlighting the need to consider the broader consequences of model deployment in dynamic, strategic environments.

# PDF

# 44. Advancing Local Clustering on Graphs via Compressive

# Sensing: Semi-supervised and Unsupervised Methods

Relevance:

arXiv ID: 2504.19419

Authors: Zhaiming Shen, Sung Ha Kang

TLDR: This paper introduces novel methods for local clustering on graphs using compressive sensing, focusing on both semi-supervised and unsupervised settings. In the semi-supervised case, it leverages limited labeled data to identify small, specific substructures (clusters) by solving a sparse linear system related to the graph Laplacian. For the unsupervised case, it employs random graph sampling, diffusion-based local cluster extraction, and overlap analysis to detect clusters without prior labels. The authors establish theoretical co-membership conditions and prove the correctness of their approach. Experimental results show state-of-the-art performance, especially under low-label regimes.

PDF

# 45. Continuous Domain Generalization

Relevance:

arXiv ID: 2505.13519

Authors: Zekun Cai, Yiheng Yao, Guangji Bai, Renhe Jiang, Xuan Song, Ryosuke Shibasaki, Liang Zhao

TLDR: This paper introduces Continuous Domain Generalization (CDG), a new task that addresses the challenge of model generalization across continuously varying domains in multiple latent factors such as time, geography, and socioeconomic contexts. Unlike traditional domain generalization methods that treat domains as discrete or unidimensional, CDG models the variation as a continuous, multidimensional manifold. The authors propose a Neural Lie Transport Operator (NeuralLio) that ensures geometric continuity and algebraic consistency in model parameter transitions across domains. To handle noisy or incomplete domain descriptors, they incorporate a gating mechanism and a local chart-based strategy. The method is evaluated on synthetic and real-world datasets—including remote sensing, scientific documents, and traffic forecasting—showing superior performance in generalization accuracy and robustness compared to existing baselines.

PDF

# 46. Optimal Online Change Detection via Random Fourier Features

Relevance:

arXiv ID: 2505.17789

Authors: Florian Kalinke, Shakeel Gavioli-Akilagun

TLDR: This paper presents a novel, truly online method for non-parametric change point detection in multivariate data streams using random Fourier features. The approach is based on kernel-based two-sample testing and achieves logarithmic time and space complexity per observation, making it highly efficient. Unlike prior methods, it does not require pre-change training data or a user-specified window parameter for local testing. The authors provide strong theoretical guarantees, including minimax optimality in detection delay, and demonstrate competitive performance on both synthetic and real-world data.

PDF

# 47. Efficient Regression-Based Training of Normalizing Flows for Boltzmann Generators

Relevance:

arXiv ID: 2506.01158

Authors: Danyal Rehman, Oscar Davis, Jiarui Lu, Jian Tang, Michael Bronstein, Yoshua Bengio, Alexander Tong, Avishek Joey Bose

TLDR: This paper introduces Regression Training of Normalizing Flows (RegFlow), a novel training method for normalizing flows in the context of Boltzmann Generators (BGs) for molecular conformation sampling. Traditional maximum likelihood training of normalizing flows is often unstable and computationally expensive, especially for high-dimensional continuous spaces like molecular systems. RegFlow replaces this with a simple  $\mathbb{S}\backslash \mathrm{ell\_2}$  -regression objective that maps prior samples to target samples derived from optimal transport couplings or a pre-trained continuous normalizing flow. The method includes a forward-backward self-consistency regularization to improve numerical stability and enables training of previously intractable architectures. Empirical results on alanine dipeptide, tripeptide, and tetrapeptide show RegFlow outperforms maximum likelihood in stability, computational efficiency, and sampling quality, making it promising for scientific applications requiring fast likelihood evaluation and sampling.

PDF

# 48. Predictive Causal Inference via Spatio-Temporal Modeling and Penalized Empirical Likelihood

Relevance:

arXiv ID: 2507.08896

Authors: Byunghee Lee, Hye Yeon Sin, Joonsung Kang

TLDR: This paper proposes a novel framework for predictive causal inference that integrates a Hidden Markov Model (HMM) for spatial health state estimation with a Multi-Task and Multi-Graph Convolutional Network (MTGCN) to model temporal outcome trajectories. The approach asymmetrically treats spatial and temporal information—using them as endogenous variables in outcome regression and exogenous variables in propensity score modeling—thereby enhancing bias correction and predictive accuracy within a doubly robust estimation framework. The method is applied to clinical domains like cancer, dementia, and Parkinson's disease, where direct observation of treatment effects is difficult. Simulation studies validate the model's performance under various latent disease dynamics. The framework is designed to handle spatiotemporal complexities in biomedical data.

PDF

# 49. Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training

Relevance:

arXiv ID: 2507.09846

Authors: Minhak Song, Beomhan Baek, Kwangjun Ahn, Chulhee Yun

TLDR: This paper investigates Schedule-Free (SF) methods for training large language models, particularly focusing on their effectiveness in large-scale, continuously scaling training scenarios. The authors analyze SF-AdamW and demonstrate that it navigates the loss landscape efficiently without relying on fixed learning rate schedules or explicit decay phases. They reveal that SF implicitly performs weight averaging without incurring additional memory costs, which addresses a key limitation of previous methods. The study also proposes a refined SF variant that improves robustness to momentum and performance under large batch sizes. Theoretical and empirical analyses support the scalability and practicality of SF as a principled alternative to conventional training strategies.

PDF

# 50. Generalized Linear Bandits: Almost Optimal Regret with One-Pass Update

Relevance:

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/cbc812b4-564b-4342-9998-8adbf88ce8f8/7ebf7633003ba6e77235df70a7219f628e25941f1bd60444be8d3257386b7e68.jpg)

arXiv ID: 2507.11847

Authors: Yu-Jie Zhang, Sheng-An Xu, Peng Zhao, Masashi Sugiyama

TLDR: This paper addresses the generalized linear bandit (GLB) problem, a contextual multi-armed bandit framework that extends linear models with non-linear link functions to model diverse reward distributions like Bernoulli and Poisson. The main challenge lies in balancing computational efficiency and statistical performance. The authors propose a novel algorithm that achieves nearly optimal regret with constant-time and constant-space updates per round. The key innovation is a tight confidence set for the online mirror descent (OMD) estimator, derived via a new analysis using mix loss from online prediction. This enables the OMD estimator to maintain statistical efficiency comparable to maximum likelihood estimation, resulting in a jointly efficient optimistic method.

PDF